{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray\n",
    "\n",
    "import IPythonMagic\n",
    "\n",
    "from Timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global logger already initialized!\n"
     ]
    }
   ],
   "source": [
    "#Initialize our logging\n",
    "\n",
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Context already registered! Ignoring\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/ipykernel_launcher.py:92: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu(7): warning: variable \"gid\" was declared but never referenced\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernel_sourcecode = \"\"\" \n",
    "\n",
    "__global__ void shmemReduction(float* output, float* input, int size){\n",
    "    // First we stride through global memory and compute the maximum for every thread\n",
    "    // gid = globalId\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x; // blockIdx is always 0 because we use just one block\n",
    "   \n",
    "   float max_thread_value = -99999.99; //FixME : use proper value here\n",
    "   \n",
    "   float max_value;\n",
    "    \n",
    "    for (int i = threadIdx.x; i < size; i = i + blockDim.x){\n",
    "    \n",
    "        max_thread_value = fmaxf(max_value, input[i]); // I compute maximum value for every thread\n",
    "        \n",
    "    }\n",
    "    \n",
    "    // Temporary write to memory to check if thigns work so far\n",
    "    \n",
    "    output[threadIdx.x] = max_thread_value; \n",
    "    \n",
    "    // Stored the pre-Thread maximum in shared memory\n",
    "    \n",
    "    __shared__ float max_shared[128];\n",
    "    \n",
    "    max_shared[threadIdx.x] = max_thread_value;\n",
    "    \n",
    "    // Sinchronize so to that all thread see the same memory\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Find the maximum in shared memory\n",
    "    \n",
    "    if (threadIdx.x < 64){\n",
    "    \n",
    "        // I have 2 warps that goes from 0 to 64 (ThreadIdx.x > 32). I need to have a sincthreads after this iteration\n",
    "    \n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 64]); // Reduce from 128 to 64 elements\n",
    "        \n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    \n",
    "    if (threadIdx.x < 32){\n",
    "    \n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 32]); // Reduce from 64 to 32 elements\n",
    "        \n",
    "    }\n",
    "    \n",
    "    if (threadIdx.x < 16){\n",
    "    \n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 16]); // Reduce from 32 to 16 elements\n",
    "        \n",
    "    }\n",
    "     if (threadIdx.x < 8){\n",
    "    \n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 8]); // Reduce from 16 to 8 elements\n",
    "        \n",
    "    }\n",
    "    \n",
    "     if (threadIdx.x < 4){\n",
    "    \n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 4]); // Reduce from 8 to 4 elements\n",
    "        \n",
    "    }\n",
    "    \n",
    "     if (threadIdx.x < 2){\n",
    "    \n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 2]); // Reduce from 4 to 2 elements\n",
    "        \n",
    "    }\n",
    "    \n",
    "     if (threadIdx.x < 1){\n",
    "    \n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x + 1]); // Reduce from 2 to 1 elements\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    // Finally write out output\n",
    "\n",
    "    if (threadIdx.x == 0){\n",
    "    \n",
    "        output[0] = max_shared[0];\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_sourcecode)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5640829  0.46394816 0.2938098  0.5588757  0.75980484 0.06264941\n",
      "  0.3925978  0.60767055 0.3309737  0.57635224 0.7923911  0.55936086\n",
      "  0.8041371  0.67112637 0.38625455 0.9260951  0.5862123  0.7016446\n",
      "  0.10683755 0.6505731  0.34600237 0.8908103  0.05418143 0.07550639\n",
      "  0.5772979  0.5965145  0.098862   0.8790986  0.97726893 0.4068067\n",
      "  0.55555344 0.73719335 0.72441036 0.65459996 0.56800693 0.6364083\n",
      "  0.10276268 0.6301994  0.08153899 0.98833483 0.3884874  0.7305744\n",
      "  0.15090898 0.4991452  0.9408307  0.7553358  0.25009578 0.00381485\n",
      "  0.49507514 0.15268004 0.78813046 0.16401848 0.29689893 0.43173113\n",
      "  0.20228119 0.646687   0.06284728 0.6491352  0.02166346 0.72434115\n",
      "  0.70255506 0.07787921 0.18926497 0.84981555 0.5543002  0.41498616\n",
      "  0.06682346 0.8694998  0.71777725 0.87532103 0.01027563 0.9601985\n",
      "  0.9111745  0.8159941  0.5331719  0.21998025 0.3731966  0.8336696\n",
      "  0.41300574 0.03510052 0.26774752 0.90999687 0.6833242  0.5786694\n",
      "  0.36977598 0.6233644  0.5213112  0.55293244 0.6281193  0.07854562\n",
      "  0.7716704  0.7535371  0.88621485 0.37933755 0.05435349 0.58937186\n",
      "  0.88903844 0.15399866 0.23255719 0.3791128  0.4855702  0.9213607\n",
      "  0.6584589  0.94966483 0.10257263 0.7708823  0.45849013 0.2757082\n",
      "  0.8083055  0.4820936  0.37957126 0.78122604 0.27347982 0.459281\n",
      "  0.38700703 0.8804758  0.19536382 0.7874498  0.78086245 0.51076555\n",
      "  0.06771071 0.5696359  0.46390986 0.67223364 0.7443861  0.3800404\n",
      "  0.01366223 0.7832283  0.153046   0.47993073 0.31620196 0.35174283\n",
      "  0.9197025  0.84823716 0.58776975 0.01000575 0.163004   0.6610079\n",
      "  0.17237587 0.14835742 0.05166182 0.52946264 0.3668703  0.38794547\n",
      "  0.01159277 0.561673   0.953289   0.76028    0.9763608  0.36689058\n",
      "  0.15038167 0.9117624  0.48224983 0.9479972  0.19412532 0.51809156\n",
      "  0.4011279  0.23302561 0.89967227 0.13861105 0.6691554  0.01830568\n",
      "  0.49491888 0.89427716 0.6339227  0.07578482 0.20945597 0.2910189\n",
      "  0.32328817 0.31003815 0.7689798  0.8063539  0.11764877 0.48707113\n",
      "  0.22781973 0.9041885  0.34797382 0.6805438  0.69468313 0.53663903\n",
      "  0.51303595 0.56144935 0.00294525 0.6010792  0.659991   0.42430174\n",
      "  0.8776301  0.39184064 0.6473692  0.6012167  0.44701818 0.03388741\n",
      "  0.42980725 0.35662985 0.4379565  0.08373047 0.7509601  0.3737005\n",
      "  0.45336154 0.19941507 0.37884197 0.66539615 0.8511808  0.5643561\n",
      "  0.6996303  0.5217307  0.04431722 0.56457466 0.6414508  0.4387143\n",
      "  0.04995269 0.3938065  0.8507803  0.08646526 0.31536657 0.4202778\n",
      "  0.638286   0.64718467 0.16875166 0.5020353  0.4521254  0.46943521\n",
      "  0.18052594 0.05383779 0.16241708 0.45573547 0.2005115  0.79416394\n",
      "  0.80038685 0.636402   0.1063815  0.82384646 0.68212575 0.9446703\n",
      "  0.5666984  0.6787353  0.92092323 0.92422694 0.53150254 0.23040773\n",
      "  0.6622713  0.7719006  0.24082907 0.00222045 0.13925336 0.29650965\n",
      "  0.7133968  0.9745978  0.069689   0.74066716 0.06744354 0.69013876\n",
      "  0.79332596 0.70576006 0.75283617 0.3553223 ]]\n"
     ]
    }
   ],
   "source": [
    "n = 256\n",
    "\n",
    "a = np.random.random((1,n)).astype(np.float32)\n",
    "\n",
    "print(a)\n",
    "\n",
    "a_gpu = GPUArray(a.shape, a.dtype)\n",
    "\n",
    "a_gpu.set(a)\n",
    "\n",
    "num_threads = 128\n",
    "\n",
    "# I start with an array 'cause at first I'm not finding maximum of all elements\n",
    "\n",
    "b = np.empty((num_threads)).astype(np.float32)\n",
    "b_gpu = GPUArray(b.shape, b.dtype)\n",
    "\n",
    "c = np.empty(1).astype(np.float32)\n",
    "c_gpu = GPUArray(c.shape, b.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5640829  0.46394816 0.2938098  0.5588757  0.75980484 0.06264941\n",
      "  0.3925978  0.60767055 0.3309737  0.57635224 0.7923911  0.55936086\n",
      "  0.8041371  0.67112637 0.38625455 0.9260951  0.5862123  0.7016446\n",
      "  0.10683755 0.6505731  0.34600237 0.8908103  0.05418143 0.07550639\n",
      "  0.5772979  0.5965145  0.098862   0.8790986  0.97726893 0.4068067\n",
      "  0.55555344 0.73719335 0.72441036 0.65459996 0.56800693 0.6364083\n",
      "  0.10276268 0.6301994  0.08153899 0.98833483 0.3884874  0.7305744\n",
      "  0.15090898 0.4991452  0.9408307  0.7553358  0.25009578 0.00381485\n",
      "  0.49507514 0.15268004 0.78813046 0.16401848 0.29689893 0.43173113\n",
      "  0.20228119 0.646687   0.06284728 0.6491352  0.02166346 0.72434115\n",
      "  0.70255506 0.07787921 0.18926497 0.84981555 0.5543002  0.41498616\n",
      "  0.06682346 0.8694998  0.71777725 0.87532103 0.01027563 0.9601985\n",
      "  0.9111745  0.8159941  0.5331719  0.21998025 0.3731966  0.8336696\n",
      "  0.41300574 0.03510052 0.26774752 0.90999687 0.6833242  0.5786694\n",
      "  0.36977598 0.6233644  0.5213112  0.55293244 0.6281193  0.07854562\n",
      "  0.7716704  0.7535371  0.88621485 0.37933755 0.05435349 0.58937186\n",
      "  0.88903844 0.15399866 0.23255719 0.3791128  0.4855702  0.9213607\n",
      "  0.6584589  0.94966483 0.10257263 0.7708823  0.45849013 0.2757082\n",
      "  0.8083055  0.4820936  0.37957126 0.78122604 0.27347982 0.459281\n",
      "  0.38700703 0.8804758  0.19536382 0.7874498  0.78086245 0.51076555\n",
      "  0.06771071 0.5696359  0.46390986 0.67223364 0.7443861  0.3800404\n",
      "  0.01366223 0.7832283  0.153046   0.47993073 0.31620196 0.35174283\n",
      "  0.9197025  0.84823716 0.58776975 0.01000575 0.163004   0.6610079\n",
      "  0.17237587 0.14835742 0.05166182 0.52946264 0.3668703  0.38794547\n",
      "  0.01159277 0.561673   0.953289   0.76028    0.9763608  0.36689058\n",
      "  0.15038167 0.9117624  0.48224983 0.9479972  0.19412532 0.51809156\n",
      "  0.4011279  0.23302561 0.89967227 0.13861105 0.6691554  0.01830568\n",
      "  0.49491888 0.89427716 0.6339227  0.07578482 0.20945597 0.2910189\n",
      "  0.32328817 0.31003815 0.7689798  0.8063539  0.11764877 0.48707113\n",
      "  0.22781973 0.9041885  0.34797382 0.6805438  0.69468313 0.53663903\n",
      "  0.51303595 0.56144935 0.00294525 0.6010792  0.659991   0.42430174\n",
      "  0.8776301  0.39184064 0.6473692  0.6012167  0.44701818 0.03388741\n",
      "  0.42980725 0.35662985 0.4379565  0.08373047 0.7509601  0.3737005\n",
      "  0.45336154 0.19941507 0.37884197 0.66539615 0.8511808  0.5643561\n",
      "  0.6996303  0.5217307  0.04431722 0.56457466 0.6414508  0.4387143\n",
      "  0.04995269 0.3938065  0.8507803  0.08646526 0.31536657 0.4202778\n",
      "  0.638286   0.64718467 0.16875166 0.5020353  0.4521254  0.46943521\n",
      "  0.18052594 0.05383779 0.16241708 0.45573547 0.2005115  0.79416394\n",
      "  0.80038685 0.636402   0.1063815  0.82384646 0.68212575 0.9446703\n",
      "  0.5666984  0.6787353  0.92092323 0.92422694 0.53150254 0.23040773\n",
      "  0.6622713  0.7719006  0.24082907 0.00222045 0.13925336 0.29650965\n",
      "  0.7133968  0.9745978  0.069689   0.74066716 0.06744354 0.69013876\n",
      "  0.79332596 0.70576006 0.75283617 0.3553223 ]]\n",
      "[0.9763608]\n",
      "0.98833483\n"
     ]
    }
   ],
   "source": [
    "block_size = (num_threads, 1, 1)\n",
    "grid_size = (1, 1, 1)\n",
    "\n",
    "kernel_function(c_gpu, a_gpu, np.int32(n), grid = grid_size, block = block_size)\n",
    "\n",
    "c_gpu.get(c)\n",
    "\n",
    "print(a)\n",
    "print(c)\n",
    "print(np.max(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
